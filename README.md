# 100-Days-of-ML
#100daysofmlchallenge
<h2>

 
| Day | Learnings |
| --- | --- |
| Day1 | -> What is Regression? <br> -> Linear Regression<br> -> Mathematics behind Linear Regression<br> -> Implementation from Scratch<br> -> Gradient Descent |
| Day2 | -> Polynomial Regression<br> -> Why Polynomial Regression?<br> -> Underfitting and Overfitting <br>  -> Implementation of Polynomial Regression |
|Day3  | -> Multiple Linear Regression<br> -> Implementation<br> -> Feature Selection in Multiple Linear Regression<br> -> Need of Data Preprocessing  |
| Day4  | -> Handling Missing Values       <h6> --->Drop Missing Values<br>   \--->Filling Missing Values<br>      \--->Prediction of Missing Values<br></h6> ->Handling Categorical Values<br>  <h6>--->One-Hot Encoding<br>         \--->Label Encoding<br>  |
| Day5 | ->Dealing with outliers<br> <h6>--->Detection Methods(Box Plot,Scatter Plot,IQR Score) </h6>->Analysis for feature Importance <br>->Feature Scaling<br> <h6> --->Z-score Normalisation,Min-Max Normalisation,Log Transformation |
|Day6 |<h4>Started Statistics <br> ->Average,Variance,Std. Deviation,Prob. Dist. Fns<br> ->Binomial Distribution<br> ->Poisson Process |
| Day7 | ->Poisson Process<br> ->Law of Large numbers<br> ->Normal Distribution |
| Day8 | ->Central Limit Theorem<br> ->Bernoulli Distribution<br>  ->Hypothesis Testing |
| Day9 | ->Hypothesis Testing(P-value,One and Two tailed Test,Z and T-statistic,Type I and Type II errors) |
| Day10 |->Covariance and correlation<br> ->chi-square distribution <br> ->ANOVA/F-statistics |
|Day11 | ->Data Visualization on Iris Dataset |
| Day12 | ->Logistic Regression<br> ->Implementation of Logistic Regression<br> ->Cross Entropy Loss<br> ->Optimization using Gradient Descent<br> ->Evaluation of Logistic Regression Result |
| Day13 | ->Ridge and Lasso Regression  |
| Day14 | ->Ridge and Lasso Regression Implementation |
| Day15 | ->Decision Trees  |
| Day16 | ->Decision Tree Implementation  |
| Day17 | ->Ensemble Methods<br> ->Bagging<br> <h6> --->Random Forest |
| Day18 | ->Ensemble Methods<br> ->Boosting<h6> --->Ada-Boost,Gradient Boosting,XGBoost
| Day19 | ->Random Forest Implementation  |
| Day20 | ->Ensemble Methods Implementation <br> Participated in kaggle competition for 1st time   [Kaggle Link](https://www.kaggle.com/ruchikamodgil)  |
| Day21 | ->Support Vector Machine |
| Day22 |   [SVM Implementation](https://github.com/Ruchikamodgil/Support-Vector-Machine-Implementation)   |
| Day23 | ->K-Nearest Neighbour|
|Day24 |->KNN Implementation|
| Day25 | ->Bayesian Learning <h6> --->Bayes Optimal Classifier,Gibb's Classifier,Naive Bayes Classifier,Gaussian Naive Bayes |
| Day26 | ->Clustering <h6> --->K-Means Algorithm |
| Day27 | ->Hierarchical Cluestering(Agglomerative) |
| Day28 | ->DBSCAN(Density Based Spatial Clustering of Applications with Noise) |
| Day29 and Day30   | ->Implementation of Clustering Algorithms in Python<br> [Clustering Algorithms](https://www.github.com/Ruchikamodgil/Clustering-Algorithms)  |
| Day31 and Day32 | Dimensionality Reduction Techniques<br> ->Feature Selection<br>  ->Feature Extraction  |
| Day33 | ->Implementation of PCA on iris dataset using Python<br>  [Principal Component Analysis](https://www.github.com/Ruchikamodgil/Principal-Component-Analysis) |
| Day34 and Day35 | -> Numpy and Pandas Revision <br> [Link](https://github.com/Ruchikamodgil/Basic-Python-Libraries) |
| Day36 | ->Revision of previous Learnings |
| Day37 and Day38 | ->Started with Maps <br> GeoSpatial Analysis  [Link](https://www.kaggle.com/ruchikamodgil/starting-with-maps) |
| Day39 | -> Getting Started with NLP |
| Day40 and Day41 | Natural Language Processing <br> -> Text Preprocessing  <h6>--->Tokenization,Stopwords,Stemming,Lemmatization,N-Grams,Bag of Words,TF-IDF (implemenatation and intution)  [Link](https://github.com/Ruchikamodgil/NLP-Natural-Language-Processing/blob/main/Text%20Preprocessing.ipynb) | 
| Day42 | -> Making Email Spam Classfier,Model which classifies the message as spam and non spam using Naive Bayes Classifier and using NLP Data Preprocessing to convert the dataset into suitable format. [Link](https://github.com/Ruchikamodgil/NLP-Natural-Language-Processing/blob/main/Email%20Spam%20Classifier.ipynb) |
| Day43 | Started with Deep Learning <br>-> Started with Neural Networks |
| Day44  | -> MultiLayer Perceptron |
| Day45 | ->Back Propagation Learning |
| Day46 | -> Gradient Descent Variations |
| Day47 | ->Started implementing NN with tensorFlow |
| Day48 | ->Fashion MNIST - Image Classification | 
| Day49 | ->Email Spam Classifier using Neural Networks <br> [Neural Networks](https://github.com/Ruchikamodgil/Neural-Networks) <br>Started with TensorFlow Input Pipelines |
| Day50 | ->Reading Input Data  [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Reading%20Input%20data-1.ipynb) |
| Day51 | ->Reading Input Data  [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Reading_Input_Data-2.ipynb)  |
| Day52 | ->Batching Dataset Elements   [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Batching.ipynb)  |
| Day53 and Day54 |  ->Shuffling and Preprocessing Data(Time Series Windowing)  [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Shuffling_And_TimeSeries_Windowing.ipynb)  |
| Day55 | ->keras Tuner - HyperParameter Tuning-Improving Spam Classifier Performance   [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/HyperParameter%20tuning%20on%20Email%20Spam%20Classifier.ipynb) |
| Day56 | ->Started with understanding RNN  [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Text_Generation_with_RNN.ipynb) |
| Day57 | ->Convolution Neural Network (CNN) |
| Day58 | ->Implementation of CNN   [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/CNN_Implementation.ipynb) |
| Day59 | Learning some Popular CNN Models <br> ->LeNet |
| Day60 | ->AlexNet,VGG16 |
| Day61 | ->Transfer Learning |
| Day62 and Day63 | Transfer Learning <br>->Cat vs Dog Classification using Transfer Learning  [Link](https://github.com/Ruchikamodgil/Neural-Networks/blob/main/Cat_vs_Dog_Classification_using_Transfer_Learning.ipynb) |
| Day64 and Day65 | ->Working on image Segmentation |
| Day66 | ->Working on image segmentation |
| Day67 and Day68 | ->Recurrent Neural Network,Exploding Gradient Descent , Vanishing Gradient Descent  |
| Day69 | ->Long Short Term Memory  |
| Day70 and Day71 | ->Gold Price Prediction using LSTM  [Link](https://github.com/Ruchikamodgil/Neural-Networks/tree/main/time%20series%20forecasting%20using%20LSTM) |
| Day72, Day73, Day74 , Day75 | ->Revised previous Learnings |
| Day76, Day77, Day78 ,Day79 | ->Started working on COVID related project (Our project got selected in semi-finals of EY Techathon) |
| Day80, Day81, Day82, Day83 | ->Semanitic Segmentation with U-net,U-net Architecture(its intution and working) <h6> --->Butterfly Segmentation using U-net [Link](https://github.com/Ruchikamodgil/Image-Segmentation-using-U-Net/blob/main/ButterFly_Segmentation.ipynb) |
| Day84, Day85, Day86 | ->Architecture of different object detection Techniques - RCNN,Fast-RCNN,Faster-RCNN  |
| Day87, Day88, Day89, Day90 | -> Time Series Forecasting  <h6> --->Autoregressive Integrated Moving Average (ARIMA) <br> --->Seasonal Autoregressive Integrated Moving-Average (SARIMA)  [Link](https://github.com/Ruchikamodgil/Time-Series-Forecasting-using-ARIMA-SARIMA)  |
| Day91, Day92 | ->Word2vec  <h6> --->Why Word2vec? <h6> --->Problems with Bag of words,TF-IDF |
| Day93, Day94, Day95 | ->MaskRCNN, Detection and Segmentation with MaskRCNN  |  
| Day96, Day97, Day98, Day99, Day100 | ->Developed on "Know-your-herb" project <h6> Done with training model on 3 different kind of herbs, will soon make the repository public after deploying my model and after developing an application which can help people to detect more exotic kitchen ingredients and herbs. | 
   
   
<h2>This was my 100 days of Machine Learning Journey. 
 
