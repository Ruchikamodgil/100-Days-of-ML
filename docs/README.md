# 100-Days-of-ML
#100daysofmlchallenge
<h2> Day1 Learnings:
<h4>-> What is Regression?<br>
-> Linear Regression<br>
-> Mathematics behind Linear Regression<br>
-> Implementation from Scratch<br>
-> Gradient Descent<br>
<h2> Day2 Learnings:
 <h4>-> Polynomial Regression<br>
 -> Why Polynomial Regression?<br>
 -> Underfitting and Overfitting <br>
 -> Implementation of Polynomial Regression
<h2>Day3 Learnings:
 <h4>-> Multiple Linear Regression<br>
 -> Implementation<br>
 -> Feature Selection in Multiple Linear Regression<br>
 -> Need of Data Preprocessing<br>
<h2>Day4 Learnings:
 <h4>->Handling Missing Values<br>
       |<br>
        --->Drop Missing Values<br>
       |<br>
        --->Filling Missing Values<br>
       |<br>
        --->Prediction of Missing Values<br>
 <h4>->Handling Categorical Values<br>
        |<br>
         --->One-Hot Encoding<br>
        |<br>
         --->Label Encoding<br>
 <h2>Day5 Learning:
  <h4>->Dealing with outliers<br>
       |<br>
        --->Detection Methods(Box Plot,Scatter Plot,IQR Score)
  <h4>->Analysis for feature Importance
  <h4>->Feature Scaling<br>
       |<br>
        --->Z-score Normalisation,Min-Max Normalisation,Log Transformation<br>
 <h2>Day6 Learnings:
  <h4>Started Statistics <br>
  ->Average,Variance,Std. Deviation,Prob. Dist. Fns<br>
  ->Binomial Distribution<br>
  ->Poisson Process<br>
 <h2>Day7 Learnings:
  <h4>->Poisson Process<br>
    ->Law of Large numbers<br>
    ->Normal Distribution<br>
 <h2>Day8 Learnings:
  <h4>->Central Limit Theorem<br>
    ->Bernoulli Distribution<br>
    ->Hypothesis Testing<br>
 <h2>Day9 Learnings:
  <h4>->Hypothesis Testing(P-value,One and Two tailed Test,Z and T-statistic,Type I and Type II errors) 
   <h2>Day10 Learnings:
  <h4>->Covariance and correlation<br>
    ->chi-square distribution <br>
    ->ANOVA/F-statistics<br>
   <h2>Day11 Learnings:
    <h4>->Data Visualization on Iris Dataset
   <h2>Day12 Learnings:
    <h4>->Logistic Regression<br>
     ->Implementation of Logistic Regression<br>
     ->Cross Entropy Loss<br>
     ->Optimization using Gradient Descent<br>
     ->Evaluation of Logistic Regression Result
     <h2>Day13 Learnings:
      <h4>->Ridge and Lasso Regression
      <h2>Day14 Learnings:
       <h4>->Ridge and Lasso Regression Implementation
     <h2>Day15 Learnings:
      <h4>->Decision Trees
     <h2>Day16 Learnings:
      <h4>->Decision Tree Implementation
     <h2>Day17 Learnings:
      <h4>->Ensemble Methods<br>
       ->Bagging<br>
       |<br>-->Random Forest
      <h2>Day18 Learnings:
       <h4>->Ensemble Methods<br>
        ->Boosting<br>
        |<br>
        -->Ada-Boost,Gradient Boosting,XGBoost
       <h2>Day19 Learnings:
        <h4>->Random Forest Implementation
       <h3>Day20 Learnings:
        <h4>->Boosting Algorithms implementation
